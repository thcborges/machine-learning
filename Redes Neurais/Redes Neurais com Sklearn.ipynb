{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n['setosa' 'versicolor' 'virginica']\n150\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(len(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13488936\nIteration 2, loss = 1.12216329\nIteration 3, loss = 1.11057126\nIteration 4, loss = 1.09984065\nIteration 5, loss = 1.08978292\nIteration 6, loss = 1.08022942\nIteration 7, loss = 1.07105334\nIteration 8, loss = 1.06217532\nIteration 9, loss = 1.05354695\nIteration 10, loss = 1.04513254\nIteration 11, loss = 1.03690666\nIteration 12, loss = 1.02885249\nIteration 13, loss = 1.02095512\nIteration 14, loss = 1.01319779\nIteration 15, loss = 1.00556087\nIteration 16, loss = 0.99802129\nIteration 17, loss = 0.99055167\nIteration 18, loss = 0.98312018\nIteration 19, loss = 0.97569105\nIteration 20, loss = 0.96822559\nIteration 21, loss = 0.96068407\nIteration 22, loss = 0.95302895\nIteration 23, loss = 0.94522943\nIteration 24, loss = 0.93726590\nIteration 25, loss = 0.92913275\nIteration 26, loss = 0.92083837\nIteration 27, loss = 0.91240246\nIteration 28, loss = 0.90385141\nIteration 29, loss = 0.89521300\nIteration 30, loss = 0.88651183\nIteration 31, loss = 0.87776635\nIteration 32, loss = 0.86898815\nIteration 33, loss = 0.86018321\nIteration 34, loss = 0.85135415\nIteration 35, loss = 0.84250216\nIteration 36, loss = 0.83362800\nIteration 37, loss = 0.82473190\nIteration 38, loss = 0.81581281\nIteration 39, loss = 0.80686767\nIteration 40, loss = 0.79789120\nIteration 41, loss = 0.78887639\nIteration 42, loss = 0.77981580\nIteration 43, loss = 0.77070312\nIteration 44, loss = 0.76153470\nIteration 45, loss = 0.75231054\nIteration 46, loss = 0.74303445\nIteration 47, loss = 0.73371359\nIteration 48, loss = 0.72435755\nIteration 49, loss = 0.71497751\nIteration 50, loss = 0.70558578\nIteration 51, loss = 0.69619626\nIteration 52, loss = 0.68682565\nIteration 53, loss = 0.67749456\nIteration 54, loss = 0.66822740\nIteration 55, loss = 0.65904991\nIteration 56, loss = 0.64998542\nIteration 57, loss = 0.64105271\nIteration 58, loss = 0.63226585\nIteration 59, loss = 0.62363351\nIteration 60, loss = 0.61515807\nIteration 61, loss = 0.60683617\nIteration 62, loss = 0.59866097\nIteration 63, loss = 0.59062538\nIteration 64, loss = 0.58272512\nIteration 65, loss = 0.57496052\nIteration 66, loss = 0.56733643\nIteration 67, loss = 0.55986023\nIteration 68, loss = 0.55253868\nIteration 69, loss = 0.54537486\nIteration 70, loss = 0.53836643\nIteration 71, loss = 0.53150593\nIteration 72, loss = 0.52478314\nIteration 73, loss = 0.51818790\nIteration 74, loss = 0.51171189\nIteration 75, loss = 0.50534812\nIteration 76, loss = 0.49908879\nIteration 77, loss = 0.49292355\nIteration 78, loss = 0.48683979\nIteration 79, loss = 0.48082489\nIteration 80, loss = 0.47486877\nIteration 81, loss = 0.46896486\nIteration 82, loss = 0.46310884\nIteration 83, loss = 0.45729655\nIteration 84, loss = 0.45152280\nIteration 85, loss = 0.44578214\nIteration 86, loss = 0.44007077\nIteration 87, loss = 0.43438736\nIteration 88, loss = 0.42873198\nIteration 89, loss = 0.42310429\nIteration 90, loss = 0.41750310\nIteration 91, loss = 0.41192758\nIteration 92, loss = 0.40637859\nIteration 93, loss = 0.40085845\nIteration 94, loss = 0.39536932\nIteration 95, loss = 0.38991227\nIteration 96, loss = 0.38448792\nIteration 97, loss = 0.37909758\nIteration 98, loss = 0.37374321\nIteration 99, loss = 0.36842642\nIteration 100, loss = 0.36314809\nIteration 101, loss = 0.35790909\nIteration 102, loss = 0.35271112\nIteration 103, loss = 0.34755634\nIteration 104, loss = 0.34244663\nIteration 105, loss = 0.33738357\nIteration 106, loss = 0.33236899\nIteration 107, loss = 0.32740512\nIteration 108, loss = 0.32249390\nIteration 109, loss = 0.31763669\nIteration 110, loss = 0.31283464\nIteration 111, loss = 0.30808905\nIteration 112, loss = 0.30340113\nIteration 113, loss = 0.29877173\nIteration 114, loss = 0.29420159\nIteration 115, loss = 0.28969161\nIteration 116, loss = 0.28524279\nIteration 117, loss = 0.28085586\nIteration 118, loss = 0.27653140\nIteration 119, loss = 0.27227006\nIteration 120, loss = 0.26807240\nIteration 121, loss = 0.26393875\nIteration 122, loss = 0.25986932\nIteration 123, loss = 0.25586430\nIteration 124, loss = 0.25192383\nIteration 125, loss = 0.24804788\nIteration 126, loss = 0.24423627\nIteration 127, loss = 0.24048885\nIteration 128, loss = 0.23680535\nIteration 129, loss = 0.23318539\nIteration 130, loss = 0.22962853\nIteration 131, loss = 0.22613435\nIteration 132, loss = 0.22270236\nIteration 133, loss = 0.21933202\nIteration 134, loss = 0.21602278\nIteration 135, loss = 0.21277403\nIteration 136, loss = 0.20958515\nIteration 137, loss = 0.20645541\nIteration 138, loss = 0.20338408\nIteration 139, loss = 0.20037039\nIteration 140, loss = 0.19741354\nIteration 141, loss = 0.19451268\nIteration 142, loss = 0.19166700\nIteration 143, loss = 0.18887565\nIteration 144, loss = 0.18613776\nIteration 145, loss = 0.18345248\nIteration 146, loss = 0.18081892\nIteration 147, loss = 0.17823621\nIteration 148, loss = 0.17570344\nIteration 149, loss = 0.17321972\nIteration 150, loss = 0.17078415\nIteration 151, loss = 0.16839583\nIteration 152, loss = 0.16605386\nIteration 153, loss = 0.16375735\nIteration 154, loss = 0.16150541\nIteration 155, loss = 0.15929716\nIteration 156, loss = 0.15713172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 157, loss = 0.15500823\nIteration 158, loss = 0.15292582\nIteration 159, loss = 0.15088365\nIteration 160, loss = 0.14888087\nIteration 161, loss = 0.14691667\nIteration 162, loss = 0.14499023\nIteration 163, loss = 0.14310073\nIteration 164, loss = 0.14124739\nIteration 165, loss = 0.13942941\nIteration 166, loss = 0.13764605\nIteration 167, loss = 0.13589652\nIteration 168, loss = 0.13418010\nIteration 169, loss = 0.13249606\nIteration 170, loss = 0.13084368\nIteration 171, loss = 0.12922227\nIteration 172, loss = 0.12763113\nIteration 173, loss = 0.12606959\nIteration 174, loss = 0.12453700\nIteration 175, loss = 0.12303270\nIteration 176, loss = 0.12155607\nIteration 177, loss = 0.12010649\nIteration 178, loss = 0.11868335\nIteration 179, loss = 0.11728606\nIteration 180, loss = 0.11591405\nIteration 181, loss = 0.11456674\nIteration 182, loss = 0.11324359\nIteration 183, loss = 0.11194406\nIteration 184, loss = 0.11066761\nIteration 185, loss = 0.10941374\nIteration 186, loss = 0.10818194\nIteration 187, loss = 0.10697172\nIteration 188, loss = 0.10578260\nIteration 189, loss = 0.10461412\nIteration 190, loss = 0.10346582\nIteration 191, loss = 0.10233725\nIteration 192, loss = 0.10122798\nIteration 193, loss = 0.10013758\nIteration 194, loss = 0.09906565\nIteration 195, loss = 0.09801177\nIteration 196, loss = 0.09697556\nIteration 197, loss = 0.09595663\nIteration 198, loss = 0.09495460\nIteration 199, loss = 0.09396912\nIteration 200, loss = 0.09299982\nIteration 201, loss = 0.09204636\nIteration 202, loss = 0.09110841\nIteration 203, loss = 0.09018562\nIteration 204, loss = 0.08927767\nIteration 205, loss = 0.08838426\nIteration 206, loss = 0.08750508\nIteration 207, loss = 0.08663982\nIteration 208, loss = 0.08578820\nIteration 209, loss = 0.08494992\nIteration 210, loss = 0.08412473\nIteration 211, loss = 0.08331233\nIteration 212, loss = 0.08251248\nIteration 213, loss = 0.08172490\nIteration 214, loss = 0.08094936\nIteration 215, loss = 0.08018560\nIteration 216, loss = 0.07943339\nIteration 217, loss = 0.07869249\nIteration 218, loss = 0.07796268\nIteration 219, loss = 0.07724373\nIteration 220, loss = 0.07653543\nIteration 221, loss = 0.07583756\nIteration 222, loss = 0.07514992\nIteration 223, loss = 0.07447231\nIteration 224, loss = 0.07380453\nIteration 225, loss = 0.07314639\nIteration 226, loss = 0.07249770\nIteration 227, loss = 0.07185827\nIteration 228, loss = 0.07122794\nIteration 229, loss = 0.07060652\nIteration 230, loss = 0.06999384\nIteration 231, loss = 0.06938974\nIteration 232, loss = 0.06879406\nIteration 233, loss = 0.06820663\nIteration 234, loss = 0.06762731\nIteration 235, loss = 0.06705593\nIteration 236, loss = 0.06649236\nIteration 237, loss = 0.06593644\nIteration 238, loss = 0.06538804\nIteration 239, loss = 0.06484702\nIteration 240, loss = 0.06431324\nIteration 241, loss = 0.06378657\nIteration 242, loss = 0.06326689\nIteration 243, loss = 0.06275406\nIteration 244, loss = 0.06224796\nIteration 245, loss = 0.06174847\nIteration 246, loss = 0.06125548\nIteration 247, loss = 0.06076887\nIteration 248, loss = 0.06028852\nIteration 249, loss = 0.05981433\nIteration 250, loss = 0.05934619\nIteration 251, loss = 0.05888399\nIteration 252, loss = 0.05842763\nIteration 253, loss = 0.05797701\nIteration 254, loss = 0.05753203\nIteration 255, loss = 0.05709260\nIteration 256, loss = 0.05665861\nIteration 257, loss = 0.05622998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.05580661\nIteration 259, loss = 0.05538842\nIteration 260, loss = 0.05497533\nIteration 261, loss = 0.05456723\nIteration 262, loss = 0.05416406\nIteration 263, loss = 0.05376573\nIteration 264, loss = 0.05337216\nIteration 265, loss = 0.05298327\nIteration 266, loss = 0.05259899\nIteration 267, loss = 0.05221924\nIteration 268, loss = 0.05184394\nIteration 269, loss = 0.05147304\nIteration 270, loss = 0.05110644\nIteration 271, loss = 0.05074410\nIteration 272, loss = 0.05038593\nIteration 273, loss = 0.05003187\nIteration 274, loss = 0.04968186\nIteration 275, loss = 0.04933583\nIteration 276, loss = 0.04899373\nIteration 277, loss = 0.04865548\nIteration 278, loss = 0.04832104\nIteration 279, loss = 0.04799033\nIteration 280, loss = 0.04766331\nIteration 281, loss = 0.04733991\nIteration 282, loss = 0.04702008\nIteration 283, loss = 0.04670377\nIteration 284, loss = 0.04639093\nIteration 285, loss = 0.04608149\nIteration 286, loss = 0.04577542\nIteration 287, loss = 0.04547265\nIteration 288, loss = 0.04517314\nIteration 289, loss = 0.04487684\nIteration 290, loss = 0.04458371\nIteration 291, loss = 0.04429370\nIteration 292, loss = 0.04400675\nIteration 293, loss = 0.04372283\nIteration 294, loss = 0.04344190\nIteration 295, loss = 0.04316390\nIteration 296, loss = 0.04288879\nIteration 297, loss = 0.04261655\nIteration 298, loss = 0.04234711\nIteration 299, loss = 0.04208045\nIteration 300, loss = 0.04181652\nIteration 301, loss = 0.04155529\nIteration 302, loss = 0.04129671\nIteration 303, loss = 0.04104075\nIteration 304, loss = 0.04078738\nIteration 305, loss = 0.04053655\nIteration 306, loss = 0.04028823\nIteration 307, loss = 0.04004239\nIteration 308, loss = 0.03979899\nIteration 309, loss = 0.03955800\nIteration 310, loss = 0.03931938\nIteration 311, loss = 0.03908311\nIteration 312, loss = 0.03884915\nIteration 313, loss = 0.03861746\nIteration 314, loss = 0.03838803\nIteration 315, loss = 0.03816081\nIteration 316, loss = 0.03793578\nIteration 317, loss = 0.03771291\nIteration 318, loss = 0.03749217\nIteration 319, loss = 0.03727354\nIteration 320, loss = 0.03705697\nIteration 321, loss = 0.03684246\nIteration 322, loss = 0.03662996\nIteration 323, loss = 0.03641946\nIteration 324, loss = 0.03621093\nIteration 325, loss = 0.03600434\nIteration 326, loss = 0.03579966\nIteration 327, loss = 0.03559688\nIteration 328, loss = 0.03539597\nIteration 329, loss = 0.03519690\nIteration 330, loss = 0.03499965\nIteration 331, loss = 0.03480420\nIteration 332, loss = 0.03461052\nIteration 333, loss = 0.03441859\nIteration 334, loss = 0.03422840\nIteration 335, loss = 0.03403991\nIteration 336, loss = 0.03385311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 337, loss = 0.03366798\nIteration 338, loss = 0.03348449\nIteration 339, loss = 0.03330263\nIteration 340, loss = 0.03312237\nIteration 341, loss = 0.03294370\nIteration 342, loss = 0.03276660\nIteration 343, loss = 0.03259104\nIteration 344, loss = 0.03241701\nIteration 345, loss = 0.03224448\nIteration 346, loss = 0.03207345\nIteration 347, loss = 0.03190390\nIteration 348, loss = 0.03173579\nIteration 349, loss = 0.03156913\nIteration 350, loss = 0.03140389\nIteration 351, loss = 0.03124005\nIteration 352, loss = 0.03107760\nIteration 353, loss = 0.03091652\nIteration 354, loss = 0.03075680\nIteration 355, loss = 0.03059841\nIteration 356, loss = 0.03044135\nIteration 357, loss = 0.03028560\nIteration 358, loss = 0.03013114\nIteration 359, loss = 0.02997795\nIteration 360, loss = 0.02982603\nIteration 361, loss = 0.02967536\nIteration 362, loss = 0.02952593\nIteration 363, loss = 0.02937771\nIteration 364, loss = 0.02923070\nIteration 365, loss = 0.02908489\nIteration 366, loss = 0.02894025\nIteration 367, loss = 0.02879678\nIteration 368, loss = 0.02865446\nIteration 369, loss = 0.02851328\nIteration 370, loss = 0.02837323\nIteration 371, loss = 0.02823429\nIteration 372, loss = 0.02809646\nIteration 373, loss = 0.02795972\nIteration 374, loss = 0.02782405\nIteration 375, loss = 0.02768945\nIteration 376, loss = 0.02755591\nIteration 377, loss = 0.02742340\nIteration 378, loss = 0.02729193\nIteration 379, loss = 0.02716148\nIteration 380, loss = 0.02703204\nIteration 381, loss = 0.02690360\nIteration 382, loss = 0.02677614\nIteration 383, loss = 0.02664967\nIteration 384, loss = 0.02652415\nIteration 385, loss = 0.02639960\nIteration 386, loss = 0.02627599\nIteration 387, loss = 0.02615332\nIteration 388, loss = 0.02603157\nIteration 389, loss = 0.02591074\nIteration 390, loss = 0.02579082\nIteration 391, loss = 0.02567180\nIteration 392, loss = 0.02555366\nIteration 393, loss = 0.02543640\nIteration 394, loss = 0.02532001\nIteration 395, loss = 0.02520448\nIteration 396, loss = 0.02508981\nIteration 397, loss = 0.02497597\nIteration 398, loss = 0.02486297\nIteration 399, loss = 0.02475080\nIteration 400, loss = 0.02463945\nIteration 401, loss = 0.02452890\nIteration 402, loss = 0.02441915\nIteration 403, loss = 0.02431020\nIteration 404, loss = 0.02420203\nIteration 405, loss = 0.02409464\nIteration 406, loss = 0.02398802\nIteration 407, loss = 0.02388216\nIteration 408, loss = 0.02377705\nIteration 409, loss = 0.02367269\nIteration 410, loss = 0.02356907\nIteration 411, loss = 0.02346618\nIteration 412, loss = 0.02336401\nIteration 413, loss = 0.02326256\nIteration 414, loss = 0.02316183\nIteration 415, loss = 0.02306179\nIteration 416, loss = 0.02296245\nIteration 417, loss = 0.02286380\nIteration 418, loss = 0.02276584\nTraining loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n-------------------------------------------------------------\nSaídas da rede:\t\t [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n 0 1 2 2 0 2 2 1]\nSaídas desejada:\t [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n 0 1 2 2 0 2 2 1]\n-------------------------------------------------------------\nScore:  1.0\nScore:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', alpha=0.0001, hidden_layer_sizes=(5,), random_state=1,\n",
    "                    learning_rate='constant', learning_rate_init=0.01, max_iter=500,\n",
    "                    activation='logistic', momentum=0.9, verbose=True, tol=0.0001)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "\n",
    "mlp.fit(x_teste, y_teste)\n",
    "saidas = mlp.predict(x_teste)\n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "print('Saídas da rede:\\t\\t', saidas)\n",
    "print('Saídas desejada:\\t', y_teste)\n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "print('Score: ', (saidas == y_teste).sum() / len(x_teste))\n",
    "print('Score: ', mlp.score(x_teste, y_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13488936\nIteration 2, loss = 1.12216329\nIteration 3, loss = 1.11057126\nIteration 4, loss = 1.09984065\nIteration 5, loss = 1.08978292\nIteration 6, loss = 1.08022942\nIteration 7, loss = 1.07105334\nIteration 8, loss = 1.06217532\nIteration 9, loss = 1.05354695\nIteration 10, loss = 1.04513254\nIteration 11, loss = 1.03690666\nIteration 12, loss = 1.02885249\nIteration 13, loss = 1.02095512\nIteration 14, loss = 1.01319779\nIteration 15, loss = 1.00556087\nIteration 16, loss = 0.99802129\nIteration 17, loss = 0.99055167\nIteration 18, loss = 0.98312018\nIteration 19, loss = 0.97569105\nIteration 20, loss = 0.96822559\nIteration 21, loss = 0.96068407\nIteration 22, loss = 0.95302895\nIteration 23, loss = 0.94522943\nIteration 24, loss = 0.93726590\nIteration 25, loss = 0.92913275\nIteration 26, loss = 0.92083837\nIteration 27, loss = 0.91240246\nIteration 28, loss = 0.90385141\nIteration 29, loss = 0.89521300\nIteration 30, loss = 0.88651183\nIteration 31, loss = 0.87776635\nIteration 32, loss = 0.86898815\nIteration 33, loss = 0.86018321\nIteration 34, loss = 0.85135415\nIteration 35, loss = 0.84250216\nIteration 36, loss = 0.83362800\nIteration 37, loss = 0.82473190\nIteration 38, loss = 0.81581281\nIteration 39, loss = 0.80686767\nIteration 40, loss = 0.79789120\nIteration 41, loss = 0.78887639\nIteration 42, loss = 0.77981580\nIteration 43, loss = 0.77070312\nIteration 44, loss = 0.76153470\nIteration 45, loss = 0.75231054\nIteration 46, loss = 0.74303445\nIteration 47, loss = 0.73371359\nIteration 48, loss = 0.72435755\nIteration 49, loss = 0.71497751\nIteration 50, loss = 0.70558578\n-------------------------------------------------------------\nSaídas da rede:\t [0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1\n 0 1 1 1 0 1 1 1]\nSaídas desejada:\t [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n 0 1 2 2 0 2 2 1]\n-------------------------------------------------------------\nScore:  0.711111111111\nScore:  0.711111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thcbo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
